
This document writes up the results of the recent run of \texttt{uberScript.R}. It contains the following code:

<<SettingOptions>>=
options(width = 88)
opts_chunk$set(tidy = FALSE)
@

<<eval=FALSE>>=
# set our simulation parameters
Replications = 100
sample.size = c(50, 100, 200, 500, 1000)
error.sd = c(2, 4, 6)
B1.spatial.var = c(0, .1, .2, .3)
B2.spatial.var = c(0, .1, .2, .3)

# now march through the different parameter combinations running the simulations

for( i in 1:meta.sim.num) { 
  start = Sys.time()
  simRepOut = simulationReplicator(Replications, sim.parameters[i, ], MC = TRUE)
  simOut = simRepReorganizer(simRepOut)
  
  R2Output[as.character(sim.parameters[i, "sample.size"]),
           as.character(sim.parameters[i, "error.sd"]),
           as.character(sim.parameters[i, "B1.spatial.var"]),
           as.character(sim.parameters[i, "B2.spatial.var"]), , ] = simOut[[1]]
  
  MetricOutput[as.character(sim.parameters[i, "sample.size"]),
               as.character(sim.parameters[i, "error.sd"]),
               as.character(sim.parameters[i, "B1.spatial.var"]),
               as.character(sim.parameters[i, "B2.spatial.var"]), , , ] = simOut[[2]]
  end = Sys.time()

  print(paste("For loop", i,"of", meta.sim.num))
  print(round(difftime(end, start, units = "m"), 2))
  save(R2Output, MetricOutput, file = "SpecificationSims/uberScriptOutput.RData")
}
@

I'm not going to run that code here (it took almost a month to run on the R Server), but let's load up the results and start to look at them. Or at least come up with some questions to ask of the data and a plan for the future.
\newpage
<<cache=TRUE>>=
load("../Data/uberScriptOutput20120919.RData")
dimnames(MetricOutput)
dimnames(R2Output)
@

So, we ran some simulations, varying the sample size of the data set, the standard deviation of the error term in the model and the degree of spatial variation in the model coefficients. 

Each simulation was conducted as follows:
\begin{enumerate}
\item Grab the simulation parameters.
\item Generate the data according to the model and parameters. 
\item Choose a number of observations to include in the Locally Weighted Regression.
\item Run Locally Weighted Regression on the data using the chosen bandwidth for each observation within the dataset.
\item Calculate a number of model metrics for each bandwidth
\item Repeat previous two steps for a number of bandwidths, ranging from only 5 data points to a model approaching a global Ordinary Least Squares model (in our case, we still had declining weights based on distance, but all observations received positive weight in the regresssion).
\item Collect data on each metric when each metric is optimized. For instance, when we choose the bandwidth associated with the lowest GCV score, what are the other metric values ($\beta$ RMSEs, etc.)
\end{enumerate}

We kept track of the following model performance metrics, the pseudo $R^2$ of the model results, the correlation between the $\hat{\beta}$ and the true $\beta$, the percent of the observations for which we can reject the null hypothesis that $\hat{\beta}=\beta$, cross validation scores (leave one out, generalized, and standardized according to Paez), lastly the AIC score.

\subsection{Data Generation Process}

The Data Generation Process is achieved using the \texttt{DataGen} function, the code for which is given below. 
<<cache=TRUE>>=
source("../SimFunctions.R")
DataGen
@

The dependent variable is produced as follows:
\begin{equation}
Y = \beta _0 + \beta _1(location) X_1 + \beta _2(location) X_2 + error
\end{equation}
where $error \sim n(0, \sigma ^2)$. Each observation is located within a geographic coordinate system $(east, north)$ where both $east$ and $north$ values are $\sim u(0, 10)$. The functions determining $\beta _1$ and $\beta _2$ are :
\begin{eqnarray}
\beta _1 (east, north) &=& 1 + Bsv1 * north - 5*Bsv1 \\
\beta _2 (east, north) &=& 1 + Bsv2 * east - 5*Bsv2
\end{eqnarray}
In our simulations we let $Bsv_i$ vary $\{0, 0.1, 0.2, 0.3\}$, thus the relationship between $\beta$s and location can be visualized as:

<<SpatVarBeta, cache=TRUE, dev='tikz', fig.height= 4, fig.width=7, include =FALSE>>=
east = north = 0:10
BetaFunc = function(x, Bsv) {
 1 + Bsv * x - 5*Bsv
}
par(mfrow = c(1, 2))
plot(north, BetaFunc(north, .3), type = "l",
     xlab = "north",
     ylab = "True Beta",
     main = expression(paste("True ", beta[1], " over Space")))
lines(north, BetaFunc(north, .2), col = "red")
lines(north, BetaFunc(north, .1), col = "blue")
lines(north, BetaFunc(north, 0), col = "orange")
text(rep(0, 4), seq(.925, -.5, length = 4), paste("Bsv1=",(0:3)/10), pos = 4,
     col = c("orange", "blue", "red", "black"), cex = .7, offset = 0)

plot(east, BetaFunc(east, .3), type = "l",
     xlab = "east",
     ylab = "True Beta",
     main = expression(paste("True ", beta[2], " over Space")))
lines(east, BetaFunc(east, .2), col = "red")
lines(east, BetaFunc(east, .1), col = "blue")
lines(east, BetaFunc(east, 0), col = "orange")
text(rep(0, 4), seq(.925, -.5, length = 4), paste("Bsv2=",(0:3)/10), pos = 4,
     col = c("orange", "blue", "red", "black"), cex = .7, offset = 0)
@

Our simulations include data generation processes in which:
\begin{enumerate}
\item neither coefficient varies over space ($Bsv_1 = 0$ \& $Bsv_2 = 0$)
\item both coefficients vary over space ($Bsv_1 \neq 0$ \& $Bsv_2 \neq 0$)
\item only one coefficient varies over space ($Bsv_1 = 0$ \& $Bsv_2 \neq 0$ OR $Bsv_1 \neq 0$ \& $Bsv_2 = 0$)
\end{enumerate}

Each simulation can be characterized by our selection of four data generation parameters,
\begin{itemize}
\item sample size $\{50, 200, 500, 1000\}$
\item variance of the error term $\{2^2, 4^2, 6^2\}$
\item degree of spatial variation in $\beta _1$  $\{0, .1, .2, .3\}$
\item degree of spatial variation in $\beta _2$  $\{0, .1, .2, .3\}$
\end{itemize}

\section{Research Questions}
What do we want to know about LWR?

\begin{enumerate}
  \item Are there systematic differences in the bandwidth size selected by different techniques? How do LOOCV, Standardaized CV, Generalize CV, and the AICc compare?
  \item What sort of spatial variation in the coefficients is necessary relative to the error to need LWR?
  \item If there is no spatial relationship, will LWR default back to global OLS?
\end{enumerate}

\section{Applying Locally Weighted Regression}

After generating the data, we applied Locally Weighted Regression and calculated numerous diagnostics in order to measure the performance of the regression technique.

Locally Weighted Regression (LWR) is an estimation strategy allowing non-stationary model parameters. A vector of regression parameters is estimated using Equation~\eqref{eq:LWR} for each location within the dataset, 
\begin{equation}\label{eq:LWR}
\hat{\beta}_{location_i} =(X^TW_{location_i}X)^{-1}X^TW_{location_i}Y,
\end{equation}
where $X$ is the standard $n$ x $m$ data matrix, $Y$ the $n$ x $1$ vector of dependent variable values, and $W_{location_i}$ is an $n$ x $n$ weights matrix. We construct the weights matrix for a given location to give positive weights to the $k$-nearest data points, with weights declining according to a bi-square function as distances increase.  Specifically, we create the weights matrix with zeros on the off-diagonal and calculate the $jj$th diagonal element as,
\begin{equation}\label{eq:bisquare}
w_{ij}= 
\begin{cases} \left[1-\left(\frac{d_{ij}}{d_{ik}}\right)^2\right]^2 & \text{if $d_{ij}\leq d_{ik}$} \\
0 &\text{if $d_{ij} > d_{ik}$}
\end{cases}
\end{equation}
where $d_{ij}$ is the distance between observations $i$ and $j$, and $d_{ik}$ is the distance to the $k$th nearest observation to observation $i$.

\subsection{Cross-Validation}
Theory does not provide guidance as to how many observations should receive positive weights in the local regression and must be determined by the researcher for the problem at hand. Typically, the $k$ parameter is determined by minimizing a cross-validation metric. This research aims to systematically compare the performance of four different cross-validation metrics used in LWR research.
\begin{enumerate}
\item Leave-One-Out Cross-Validation
\item Generalized Cross-Validation 
\item Standardized Cross-Validation
\item Akaike Information Criterion
\end{enumerate}

Does choosing the optimal number of observations to include in the LWR through these four strategies yield similar results? If there are differences, are there patterns in how they are different?

\subsection{Leave-One Out Cross-Validation}

\begin{equation}
\sum (y - \hat{y}_{-i})^2
\end{equation}

\subsection{Generalized Cross-Validation Score}
\begin{equation}\label{eq:GCV}
  n*\sum_{i=1}^{n}\frac{(y_i-\hat{y}_i)^2}{(n-v_1)^2}, 
  \end{equation}
where $y_i$ is the dependent variable value, $\hat{y}_i$ is the predicted dependent variable value for observation $i$, and $v_1$ is the ``effective number of model parameters.''\footnote{
  $v_1=$tr(\textbf{S}), where the matrix \textbf{S} is the ``hat matrix'' which maps $y$ onto $\hat{y}$,
\begin{equation*}
  \hat{y}=\textbf{S}y,
  \end{equation*}
  and each row of \textbf{S}, $r_i$ is given by:
  \begin{equation*}
    r_i=X_i(X^TW(location_i)X)^{-1}X^TW(location_i).
    \end{equation*}
}
In an LWR model, the number of parameters to be estimated is no longer equal to the number of variables included because we allow the regression coefficients to vary over space. The GCV score calculates the ``effective'' number of model parameters, $v_1$, and penalizes the model for increasing the number of parameters without sufficient reduction in model accuracy. Taking the square root of Equation~\eqref{eq:GCV} and rearranging yields,
\begin{equation}
  \sqrt{GCV}=\sqrt{\frac{n}{n-v_1}} \sqrt{\frac{\textrm{Sum of Squared Residuals}}{n-v_1}},
\end{equation}
which approaches $\hat{\sigma}$ as $v_1$ approaches $m$ for large $n$. Henceforth, throughout the paper we report the square root of \eqref{eq:GCV} because of its similarity to $\hat{\sigma}$.  

\subsection{Row Standardized Cross-Validation}

Something about Paez, who wanted a CV score that was more robust to outliers.

\begin{equation}\label{eq:SCV}
\frac{\sum (y - y_{-i})^2} {\sum y}
\end{equation}

\subsection{Akaike Information Criterion}

\begin{equation}\label{eq:AIC}
  2*n*ln(\hat{\sigma}) + n*ln(2*\pi) + 
    n*\frac{n + v_1}{n - 2 - v_1}
    \end{equation}
    
\section{Which Bandwidths Do Selection Metrics Suggest?}

In this section we compare the bandwidth selected by the different metrics.

\subsection{Overall}

\begin{itemize}
\item What are some summary stats about the bandwidths selected by each metric? (table: row for each metric, column for min, median, mean, max, sd)
\item What is the visual distribution of bandwidths selected by each metric? (small multiples of a histogram for each metric)
\end{itemize}

<<cache=TRUE>>=
mymetrics = c("CV", "GCV", "SCV", "AICc")
summary.table = matrix(0, length(mymetrics), 7)
for (mymetric in mymetrics) {
  summary.table[which(mymetrics == mymetric), 1:6] = 
                summary(MetricOutput[ , , , , , mymetric, "bandwidths"])
  summary.table[which(mymetrics == mymetric), 7] = 
                sd(MetricOutput[ , , , , , mymetric, "bandwidths"])
}
rownames(summary.table) = mymetrics
colnames(summary.table) = c("min", "Q1", "median", "mean", "Q3", "max", "sd")
print(round(summary.table))
@



Comparing the bandwidths at this level of aggregation is of limited use because we do not expect the same bandwidth to always be suggested. First, the bandwidth suggestion is constrained to be smaller than the sample size of the data, and so we should break out the simulation. Second, we expect the bandwidth selected to be a function of the degree of spatial variation in the underlying data generation process.

\subsection{Sample Size}

<<cache=TRUE>>=
myss = c("50", "100", "200", "500", "1000")
mymetrics = c("CV", "GCV", "SCV", "AICc")

for (ssi in myss) {
  summary.table = matrix(0, length(mymetrics), 7)
  for (mymetric in mymetrics) {
    summary.table[which(mymetrics == mymetric), 1:6] = 
                  summary(MetricOutput[ ssi, , , , , mymetric, "bandwidths"])
    summary.table[which(mymetrics == mymetric), 7] = 
                  sd(MetricOutput[ ssi, , , , , mymetric, "bandwidths"])
  }
  rownames(summary.table) = mymetrics
  colnames(summary.table) = c("min", "Q1", "median", "mean", "Q3", "max", "sd")
    print(paste("Sample Size =", ssi))
    print(round(summary.table))
}

@

<<beanplots, dev='tikz', cache=TRUE, fig.height = 8, fig.width = 6, include=FALSE>>=
require(beanplot)
require(RColorBrewer)
mypal = brewer.pal(4, "Set2")

par(mfrow = c(3, 2))
par(oma = c(0, 0, 2, 0))
par(mar = c(2, 4.5, 3, 0))
myss = c("50", "100", "200", "500", "1000")
bws = c(2.5, 2.5, 5, 5, 10)
myssi = "1000"
for (myssi in myss) {
  beanplot(MetricOutput[ myssi, , , , , mymetrics[1], "bandwidths"],
           MetricOutput[ myssi, , , , , mymetrics[2], "bandwidths"],
           MetricOutput[ myssi, , , , , mymetrics[3], "bandwidths"],
           MetricOutput[ myssi, , , , , mymetrics[4], "bandwidths"],
           what = c(0, 1, 1, 0) , 
           log = "",
           bw = bws[which(myss == myssi)], 
           cutmin = 5, 
           cutmax = as.numeric(myssi),
           ylim = c(0, as.numeric(myssi)),
           xlim = c(0.5, 4.5),
           names = FALSE,
           main = paste("Sample Size = ", myssi),
           ylab = "",
           col = list(col = mypal[1], col = mypal[2], col = mypal[3], col = mypal[4]), 
           axes = FALSE)
  mtext(mymetrics, 1, line = 0, at = 1:4, col= mypal, font = 2)
  mtext("\\# of obs in LWR bandwidth", 2, line = 3, cex = .8)
  axis(2, las = 1)
  mtext("Bandwidth Distributions by Metric and Sample Sizes", 3, 
        outer = TRUE, line = 0, cex = 1.5)

}
@
\includegraphics[width = .9\textwidth]{figure/beanplots.pdf}

Notice that the distributions of selected bandwidths are similar for the CV, GCV, and AICc metrics, while the SCV metric distribution stands out, especially at higher bandwidths. Additionally, note that most distributions have a cluster of selected bandwidths near the sample size. Given that one simulation parameterization included no spatial variation within the data generation coefficients, it makes sense to see a cluster of large bandwidths, a model specification that approaches Ordinary Least Squares. We now proceed to show the distributions by degree of spatial variation in the model coefficients. Figure \ref{fig:beanplots}.

\subsection{By Degree of Coefficient Variation}

Challenge: We used four different levels of spatial variation in each of our two model coefficients, giving us a total of 16 spatial variation cases.

<<MegaFig, dev='tikz', cache=TRUE, fig.height= 7, fig.width=7, include=FALSE>>=
# The goal of this code is to make a 4 x 4 gird of beanplots showing the distributions of optimal bandwidths across the four different LWR metrics and the combinations of Bsv parameters. 

require(RColorBrewer)
require(beanplot)
mypal = brewer.pal(4, "Set2")

# set some figure margin parameters

my.B1 = my.B2 = 1

df = layout( matrix(c(0, rep(17, 4),
                      18, 1:4,
                      18, 5:8,
                      18, 9:12,
                      18, 13:16), 5, 5, byrow = T),
             widths = c(.6, rep(1, 4)),
             heights = c(.6, rep(1, 4)))

#layout.show(df)

myssi = "1000"
myss = c("50", "100", "200", "500", "1000")
mymetrics = c("CV", "GCV", "SCV", "AICc")
bws = c(2.5, 2.5, 5, 5, 10)

par(oma = c(0, 0, 0, 2.5))
par(mar = c(.5, .5, .5, .5))

for (my.B2 in 1:4){ # four because there are four different B2sv parameter files
  for (my.B1 in 1:4) { # four because there are four different B1sv parameter files
    # now make a beanplot for the given Bsv1 and Bsv2
    beanplot(MetricOutput[myssi, , my.B1, my.B2, , mymetrics[1], "bandwidths"],
             MetricOutput[myssi, , my.B1, my.B2, , mymetrics[2], "bandwidths"],
             MetricOutput[myssi, , my.B1, my.B2, , mymetrics[3], "bandwidths"],
             MetricOutput[myssi, , my.B1, my.B2, , mymetrics[4], "bandwidths"],
             what = c(0, 1, 1, 0) , 
             log = "",
             bw = bws[which(myss == myssi)], 
             cutmin = 5, 
             cutmax = as.numeric(myssi),
             ylim = c(0, as.numeric(myssi)),
             main = "",
             axes = FALSE,
             col = list(col = mypal[1], col = mypal[2], col = mypal[3], col = mypal[4]))
    if(my.B1 == 1) axis(2, las = 1) #, at = seq(0, as.numeric(myssi), l = 5)
    if(my.B1 == 4) axis(4, las = 1)
  }
}

# Now work on the column labels
par(mar = c(0, 0, 0, 0))
spots = seq(.125, .875, l = 4)
plot(1, xaxs="i", xlim = c(0, 1), yaxs = "i", ylim = c(0, 1), type = "n", axes = F)
axis(1, line = -2, at = spots, 
     labels = F, col = "red")
text(spots, rep(0, 4), c("none", "some", "more", "most"), 
     col = "red", pos = 3, cex = 1.3)
text(.5, .4, "Degree of Spatial Variation in $\\beta_1$", 
     col = "red", cex = 1.5, font = 2)
#points((0:100)/100, rep(0, 101), col = c("red", rep("black", 9)))

# Now work on the row labels
plot(1, xaxs="i", xlim = c(0, 1), yaxs = "i", ylim = c(0, 1), type = "n", axes = F)
axis(4, line = -5, at = spots, 
     labels = F, col = "red")
text(rep(.5, 4), spots, c("most", "more", "some", "none"), 
     col = "red", cex = 1.3, srt = 90)
text(.2, .5, "Degree of Spatial Variation in $\\beta_2$", 
     col = "red", cex = 1.5, srt = 90)

# Title and Legend
mtext(paste("Bandwidth Distributions by Metric and Degree of Spatial Variation"), 
      outer = TRUE, line = -2.5, side = 3, font = 2, cex = 1, at = .52)
mtext(c("Metric =", "CV", "GCV", "SCV", "AICc"), outer = TRUE, line = -23, side = 3, 
      cex = .8, at = c(.29, .37, .41, .46, .508), adj = 0, 
      col = c("black", mypal), font = 2)
@

\includegraphics[width = .9\textwidth]{figure/MegaFigFinal.pdf}


<<TestChild, child='results.Rnw'>>=
@